---
title: Avoiding leaks on The Titanic
author: ''
date: '2019-10-20'
slug: avoiding-leaks-on-the-titanic
categories: []
tags: []
description: ''
featured: ''
featuredalt: ''
featuredpath: ''
linktitle: ''
---

I've been obsessed with data leakage. Paranoid, even.

Here's how it *should* work: you chop your data up into a train and test set. Then you set your test set aside and pretend that it doesn't exist; you pretend it's new data that hasn't come in yet. Then you manipulate your training data, engineer some features, and train a fancy model. Finally, you remember that test set you forgot you had, and use that to determine whether your fancy model is a good one.

Sometimes you'll do some cross-validation, but it's the same idea; you're just chopping your data up into a train and test set multiple times. The end result is the same: it's harder to overfit on data your model hasn't seen.

But it's not a perfect process. *Data leakage* occurs when your model gets a peak at the test data, without directly being trained on the test data. This can happen for tricky reasons, like maybe your data contains groups in some way, and you need to make sure that if one point in a group is in the train/test set, then the rest of that group is as well. But it can also happen due to sloppiness --- you've accidentally done something that pulls information from your test set into your train set.

Here's an *extremely common* example: your data contains some missing values. You decide to use mean-imputation, to fill the missing data with the variable mean before splitting into train and test. Your mean is likely different to what it would have been if you had split into train and test first. When this happens, you can no longer trust the performance of your model on the test set, because that data is no longer *unseen*.

We're going to revisit a classic data set --- [the Titanic survival data](https://www.kaggle.com/c/titanic/data) --- with a focus on avoiding data leakage. We'll be using some great packages that are well worth checking out: 

* `rsample` is what we'll use to split our data up into test and train. We'll also use it set up cross-validation in a way that avoids data leakage.
* `recipes` lets us manipulate data on the training set while also being able to apply that same manipulation to the test set without having first seen it.
* `parsnip` gives a unified interface for training models, and is designed to work really well with `rsample` and `recipes`. One of my biggest gripes with R is its inconsistent modelling APIs, and `parsnip` aims to fix this.

# Data setup

```{r hidden_setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(width = 80)
```

Let's get some things set up.

```{r setup, message = FALSE}
library(tidyverse)
library(rsample)
library(recipes)
library(parsnip)

set.seed(21545732)

# nifty function for making our table output pretty
output_table <- function(dat) {
  knitr::kable(dat, format = "html") %>% 
  kableExtra::kable_styling(full_width = F)
}

# R has no built-in mode function
# Copied from https://stackoverflow.com/a/8189441
# This function isn't displayed in the outputted file, so we hide it as a 
# setup step.
mode_avg <- function(x) {
  ux <- unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}
```

# Data load

We'll load our `train` and `test` data sets with some preparation. First we'll convert the column names to snake case. We'll also convert `age` and `pclass` (passenger class) to factors, as this will be needed for some data transformation steps later on. We'll also take `survived` as a factor (if it exists) as this is better for plotting.

This is as much as we'll manipulate the test set before we forget about it. We can change column names, since these are independent of contents of the data. And by specifying the levels of the factors, we're not assuming anything about the values. The guiding question is this: what happens if, after we've written all of our code, we get a brand new single observation? Can our data manipulation code handle it?

```{r data_load, message=FALSE}
data_clean <- function(data) {
  data %>% 
    janitor::clean_names(case = "snake") %>% 
    rename(sibsp = sib_sp) %>% # Keep this consistent with par_ch
    mutate(
      sex = factor(sex, levels = c("female", "male")),
      pclass = factor(pclass, levels = c(1, 2, 3))
    ) %>% 
    mutate_at(if("survived" %in% names(.)) "survived" else integer(0), function(x) factor(x, c(0, 1)))
}

train <- readr::read_csv("data/titanic/train.csv") %>% data_clean
test <- readr::read_csv("data/titanic/test.csv") %>% data_clean
```

I never miss an opportunity to use the `visdat` package, and `skimr` is a handy tool as well. These will reveal some missing values in the `r knitr::combine_words(naniar::miss_var_which(train), before = '\x60')` columns. We'll deal with these missing values later.

```{r visdat_train}
visdat::vis_dat(train)
```

```{r skimr_train, results = "asis"}
skimr::skim(train) %>% skimr::kable()
```

# Feature engineering

You might not think of feature engineering as an opportunity for data leakage, but certain operations can leak information. For example, if you're collapsing categorical variables such that any value appearing in fewer than 5% of the observations is considered "other", then the values your categories can take can be directly altered by the observations in your test set.

## Titles

The names here contain titles, and those titles may prove valuable to our modelling efforts:

```{r random_names}
train %>% sample_n(10) %>% select(name) %>% output_table
```

Let's extract the titles with a bit of regex and count them:

```{r title_counts}
train %>% 
    mutate(title = stringr::str_extract(name, "([A-Za-z]+)(?=\\.)")) %>% 
    count(title, sort = TRUE) %>% 
    output_table
```

Most of the titles are "Mr", "Miss" or "Mrs". We also have a few people with a "Master" title, as well as some academic, military and noble titles. Since the last few categories are infrequently occurring, we'll collapse them all into an "other" category. We do this with the `step_other` recipe step, which sets all values that occur less frequently than a given threshold to be set to "other". We'll set our threshold at `r scales::percent(0.04)` so that we can include the "Master" title---this title is generally reserved for boys, so it may be relevant to survival.

We'll keep track of our data manipulation---such as feature engineering and missing data imputation---with the `recipes` package. This package allows us to define methods of data transformation based on one data set and apply those methods to the another data set. This is similar to the `fit_transform` and `transform` methods common in python syntax.

First we'll define a blank recipe based on the data set, predicting for `survived` as a function of all other variables:

```{r recipe_initial_definition}
recipe <- recipe(survived ~ ., data = train)
```

Now we can add *steps* to this recipe. Steps chain together various transformations of the data, including both feature engineering and missing value imputation.

```{r title_step}
recipe <- recipe %>% 
    step_mutate(
        title = stringr::str_extract(name, "([A-Za-z]+)(?=\\.)")
    ) %>% 
    step_other(title, threshold = 0.04)
recipe
```

Let's see what would happen if we were to use this recipe as it is, with just the one step. The language is *adorable*---we `prep` the recipe on the training data and then `bake` it on new data. Normally our new data would be the test set, but for this example we'll use the training set to see what happens.

```{r example_baking_titles}
recipe %>% 
    prep(training = train, strings_as_factors = FALSE) %>% 
    bake(new_data = train) %>% 
    count(title, sort = TRUE) %>% 
    output_table
```

## Family size

Two variables here have abbreviated names, and so require a bit of explaining: `sipsp` gives the number of siblings and spouses on board for each customer, and `parch` the number of parents and children. We add these together, along with `1` for the passenger themselves, to describe the size of the family on board. 

This operation is a simple bit of arithmetic, and isn't likely to lead to data leakage. But it's an important piece of feature engineering, and a good way to show off the `step_mutate` function of the `recipes` package.

```{r family_size_step}
recipe <- recipe %>% 
    step_mutate(
        family_size = sibsp + parch + 1
    )
```

Let's see how the big the families on board were. We'll overlay this information with the survival data to see if certain sizes of families are more likely to survive.

```{r example_baking_family_size}
recipe %>% 
    prep(training = train, strings_as_factors = FALSE) %>% 
    bake(new_data = train) %>% 
    ggplot(aes(x = family_size, fill = survived, group = survived)) + 
    geom_bar(position = "dodge") + 
    ggtitle("Survival of families aboard the Titanic") 
```

We see here that people travelling by themselves are much less likely to survive. Those in families of size 2 or 3 are more likely to survive. This may tie into the notion of "women and children first", as children won't be travelling alone, and `r solo<-filter(train,sibsp==0&parch==0);scales::percent(nrow(filter(solo,sex=="male"))/nrow(solo))` of solo travellers are male.
