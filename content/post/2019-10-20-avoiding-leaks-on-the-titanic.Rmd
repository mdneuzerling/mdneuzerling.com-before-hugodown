---
title: Avoiding leaks on The Titanic
author: ''
date: '2019-10-20'
slug: avoiding-leaks-on-the-titanic
categories: []
tags: []
description: ''
featured: ''
featuredalt: ''
featuredpath: ''
linktitle: ''
---

I've been obsessed with data leakage. Paranoid, even.

Here's how it *should* work: you chop your data up into a train and test set. Then you set your test set aside and pretend that it doesn't exist; you pretend it's new data that hasn't come in yet. Then you manipulate your training data, engineer some features, and train a fancy model. Finally, you remember that test set you forgot you had, and use that to determine whether your fancy model is a good one.

Sometimes you'll do some cross-validation, but it's the same idea; you're just chopping your data up into a train and test set multiple times. The end result is the same: it's harder to overfit on data your model hasn't seen.

But it's not a perfect process. *Data leakage* occurs when your model gets a peak at the test data, without directly being trained on the test data. This can happen for tricky reasons, like maybe your data contains groups in some way, and you need to make sure that if one point in a group is in the train/test set, then the rest of that group is as well. But it can also happen due to sloppiness --- you've accidentally done something that pulls information from your test set into your train set.

Here's an *extremely common* example: your data contains some missing values. You decide to use mean-imputation, to fill the missing data with the variable mean before splitting into train and test. Your mean is likely different to what it would have been if you had split into train and test first. When this happens, you can no longer trust the performance of your model on the test set, because that data is no longer *unseen*.

We're going to revisit a classic data set --- [the Titanic survival data](https://www.kaggle.com/c/titanic/data) --- with a focus on avoiding data leakage. We'll be using some great packages that are well worth checking out: 

* `rsample` is what we'll use to split our data up into test and train. We'll also use it set up cross-validation in a way that avoids data leakage.
* `recipes` lets us manipulate data on the training set while also being able to apply that same manipulation to the test set without having first seen it.
* `parsnip` gives a unified interface for training models, and is designed to work really well with `rsample` and `recipes`. One of my biggest gripes with R is its inconsistent modelling APIs, and `parsnip` aims to fix this.

# Data setup

```{r hidden_setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(width = 80)
```

Let's get some things set up.

```{r setup, message = FALSE}
library(tidyverse)
library(rsample)
library(recipes)
library(parsnip)

set.seed(21545732)

# nifty function for making our table output pretty
output_table <- function(dat) {
  knitr::kable(dat, format = "html") %>% 
  kableExtra::kable_styling(full_width = F)
}

# R has no built-in mode function
# Copied from https://stackoverflow.com/a/8189441
# This function isn't displayed in the outputted file, so we hide it as a 
# setup step.
mode_avg <- function(x) {
  ux <- unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}
```

# Data load

For those unfamiliar with Kaggle competitions, the data comes already split into train and test. The test set does not contain the target variable, `survived`. Instead, the test set is scored and the scores uploaded to Kaggle, where the results are judged without revealing the "answers". To judge any models we train ourselves we'll need to split out training set up into another train and test. 

We'll load our `train` and `test` data sets with some preparation. First we'll convert the column names to snake case. We'll also convert `age` and `pclass` (passenger class) to factors, as this will be needed for some data transformation steps later on. We'll also take `survived` as a factor (if it exists) as this is better for plotting.

This is as much as we'll manipulate the test set before we forget about it. We can change column names, since these are independent of contents of the data. And by specifying the levels of the factors, we're not assuming anything about the values. The guiding question is this: what happens if, after we've written all of our code, we get a brand new single observation? Can our data manipulation code handle it?

```{r data_load, message=FALSE}
data_clean <- function(data) {
  data %>% 
    janitor::clean_names(case = "snake") %>% 
    rename(sibsp = sib_sp) %>% # Keep this consistent with par_ch
    mutate(
      sex = factor(sex, levels = c("female", "male")),
      pclass = factor(pclass, levels = c(1, 2, 3))
    ) %>% 
    mutate_at(if("survived" %in% names(.)) "survived" else integer(0), function(x) factor(x, c(0, 1)))
}

train <- readr::read_csv("data/titanic/train.csv") %>% data_clean
test <- readr::read_csv("data/titanic/test.csv") %>% data_clean
```

I never miss an opportunity to use the `visdat` package, and `skimr` is a handy tool as well. These will reveal some missing values in the `r knitr::combine_words(naniar::miss_var_which(train), before = '\x60')` columns. We'll deal with these missing values later.

```{r visdat_train}
visdat::vis_dat(train)
```

```{r skimr_train, results = "asis"}
skimr::skim(train) %>% skimr::kable()
```

# Feature engineering

You might not think of feature engineering as an opportunity for data leakage, but certain operations can leak information. For example, if you're collapsing categorical variables such that any value appearing in fewer than 5% of the observations is considered "other", then the values your categories can take can be directly altered by the observations in your test set.

## Titles

The names here contain titles, and those titles may prove valuable to our modelling efforts:

```{r random_names}
train %>% sample_n(10) %>% select(name) %>% output_table
```

Let's extract the titles with a bit of regex and count them:

```{r title_counts}
train %>% 
    mutate(title = stringr::str_extract(name, "([A-Za-z]+)(?=\\.)")) %>% 
    count(title, sort = TRUE) %>% 
    output_table
```

Most of the titles are "Mr", "Miss" or "Mrs". We also have a few people with a "Master" title, as well as some academic, military and noble titles. Since the last few categories are infrequently occurring, we'll collapse them all into an "other" category. We do this with the `step_other` recipe step, which sets all values that occur less frequently than a given threshold to be set to "other". We'll set our threshold at `r scales::percent(0.04)` so that we can include the "Master" title---this title is generally reserved for boys, so it may be relevant to survival.

We'll keep track of our data manipulation---such as feature engineering and missing data imputation---with the `recipes` package. This package allows us to define methods of data transformation based on one data set and apply those methods to the another data set. This is similar to the `fit_transform` and `transform` methods common in python syntax.

First we'll define a blank recipe based on the data set, predicting for `survived` as a function of all other variables:

```{r recipe_initial_definition}
recipe <- recipe(survived ~ ., data = train)
```

Now we can add *steps* to this recipe. Steps chain together various transformations of the data, including both feature engineering and missing value imputation.

```{r title_step}
recipe <- recipe %>% 
    step_mutate(
        title = stringr::str_extract(name, "([A-Za-z]+)(?=\\.)")
    ) %>% 
    step_other(title, threshold = 0.04)
recipe
```

Let's see what would happen if we were to use this recipe as it is, with just the one step. The language is *adorable*---we `prep` the recipe on the training data and then `bake` it on new data. Normally our new data would be the test set, but for this example we'll use the training set to see what happens.

```{r example_baking_titles}
recipe %>% 
    prep(training = train, strings_as_factors = FALSE) %>% 
    bake(new_data = train) %>% 
    count(title, sort = TRUE) %>% 
    output_table
```

## Family size

Two variables here have abbreviated names, and so require a bit of explaining: `sipsp` gives the number of siblings and spouses on board for each customer, and `parch` the number of parents and children. We add these together, along with `1` for the passenger themselves, to describe the size of the family on board. 

This operation is a simple bit of arithmetic, and isn't likely to lead to data leakage. But it's an important piece of feature engineering, and a good way to show off the `step_mutate` function of the `recipes` package.

```{r family_size_step}
recipe <- recipe %>% 
    step_mutate(
        family_size = sibsp + parch + 1
    )
```

Let's see how the big the families on board were. We'll overlay this information with the survival data to see if certain sizes of families are more likely to survive.

```{r example_baking_family_size}
recipe %>% 
    prep(training = train, strings_as_factors = FALSE) %>% 
    bake(new_data = train) %>% 
    ggplot(aes(x = family_size, fill = survived, group = survived)) + 
    geom_bar(position = "dodge") + 
    ggtitle("Survival of families aboard the Titanic") 
```

We see here that people travelling by themselves are much less likely to survive. Those in families of size 2 or 3 are more likely to survive. This may tie into the notion of "women and children first", as children won't be travelling alone, and `r solo<-filter(train,sibsp==0&parch==0);scales::percent(nrow(filter(solo,sex=="male"))/nrow(solo))` of solo travellers are male.

# Missing values

Now let's take care of those missing `r knitr::combine_words(naniar::miss_var_which(train), before = '\x60')` values we noted earlier.

## Imputation of `embarked`

We'll start with the variable with the smallest non-zero number of missing values, `embarked`. This gives the port of embarkation for each customer. It can take on three values:

```{r embarked_value_count}
train %>% 
    count(embarked, sort = TRUE) %>% 
    left_join(
        tribble(
            ~embarked, ~port,
            "C",       "Cherbourg",
            "Q",       "Queenstown",
            "S",       "Southampton"
        ),
        by = "embarked"
    ) %>% 
    select(embarked, port, n) %>% 
    output_table
```

With only `r sum(is.na(train$embarked))` missing values, we shouldn't spend too much time on this variable. There's also a clear majority here, with `r mode_avg(train$embarked)` being the most common value. A quick way to deal with these missing values is to impute with the mode, so that every missing value is assumed to be `r mode_avg(train$embarked)`.

The `recipes` package is especially useful here, as to avoid data leakage the imputation cannot consider the test set when defining the mode. That is, we define the mode of `embarked` based on the `train` set and then use this value to impute missing values in the `test` set.

```{r recipe_definition_and_embarked_impute}
recipe <- recipe %>% 
    step_modeimpute(embarked)
recipe
```

Let's take a look at how this recipe imputes missing data:

```{r example_baking}
recipe %>% 
    prep(training = train, strings_as_factors = FALSE) %>% 
    bake(new_data = train) %>% 
    count(embarked, sort = TRUE) %>% 
    output_table
```

As expected, those two missing values became `S`.

## Imputation of `fare`

There are no missing values of `fare` in the `train` set. However, if we peek ahead to the `test` set, we see that `r sum(is.na(test$fare))` point has a missing `fare` value. So we want to be able to handle missing values.

Wait, hang on, we're supposed to be pretending that test set doesn't exist, aren't we? Well I imagine that we would find out about these missing values when we load the data set and errors occur. And this is a great example of using `recipes` to avoid data leakage --- we define an imputation strategy for the missing `fare` values in the test set without even looking at the test set.

We might (sensibly) presume that fares are related to the class in which the passenger is travelling:

```{r fare_density}
train %>% 
    filter(fare <= 100) %>% 
    ggplot(aes(x = pclass, y = fare, fill = pclass)) +
        geom_violin(alpha = 0.25) + 
        ggtitle("Fare violin plots")
```

In fact we can break this down further to include ticket fares based on `sex` and `pclass`.

```{r fare_breakdown_by_sex_pclass}
train %>% 
    group_by(sex, pclass) %>% 
    summarise(
        passengers = n(),
        mean_fare = round(mean(fare, na.rm = TRUE), 1),
    ) %>% 
    output_table
```

We'll use k-nearest neighbours to impute any missing fares, so that the imputed age values are considerate of `sex` and `pclass`. We'll do this by using the `recipes` package again. Since we've already defined the `recipe` when imputing `embarked`, we can take that recipe and add the imputation step.

Unfortunately, this step only works if `sex` and `pclass` are factors, not strings. For more information, see [the relevant GitHub issue](https://github.com/tidymodels/recipes/issues/213). Fortunately, we took care of this requirement when loading the data.

```{r recipe_fare_impute}
recipe <- recipe %>% 
    step_knnimpute(
        fare, 
        impute_with = c("sex", "pclass"), 
        neighbors = 10
    )
recipe
```

Note that we're able to view the steps for this recipe along the way, in a human-readable form.

## Imputation of `age`

We'll take a bit more care with the `r scales::percent(sum(is.na(train$age))/nrow(train))` of `age` values that are missing. Let's start by taking a look at how the values are distributed (when present):

```{r age_density}
train %>% 
    filter(!is.na(age)) %>% 
    ggplot(aes(x = age, fill = survived)) +
        geom_density(alpha = 0.25) + 
        scale_y_continuous(labels = scales::percent) +
        ggtitle("Age density plot")
```

We can see a tail in the age here, with older passengers skewing the mean age to be slightly higher than the median.

```{r age_summary}
summary(train$age)
```

With this in mind, we might be tempted to impute missing age values with the median value of `r round(median(train$age, na.rm = TRUE), 2)`. However, if we take a look at the age variable by `sex`, `pclass` and `fare`, it looks as though age is not missing at random:

```{r age_breakdown_by_sex_pclass}
train %>% 
    group_by(sex, pclass) %>% 
    summarise(
        passengers = n(),
        mean_age = round(mean(age, na.rm = TRUE), 1),
        mean_fare = round(mean(fare, na.rm = TRUE), 1),
        missing_age = scales::percent(sum(is.na(age)) / passengers),
        survived = scales::percent(mean(survived == 1))
    ) %>% 
    output_table
```

Passengers are more likely to survive if they are in women travelling in 1st class with more expensive fares, and this category of passengers has an older mean age. If we impute with the median value, this information would be lost.

We'll use k-nearest neighbours to impute our missing ages instead, so that the imputed age values are considerate of `sex`, `pclass` and `fare`. We'll also consider title here, as some titles such as "Mrs" and "Master" may be relevant to age.

```{r recipe_age_impute}
recipe <- recipe %>% 
    step_knnimpute(
        age, 
        impute_with = c("sex", "pclass", "fare", "title"), 
        neighbors = 10
    )
recipe
```

Again, let's look at the impact of this imputation:

```{r age_density_compared_to_imputed_values}
recipe %>% 
    prep(training = train, strings_as_factors = FALSE) %>% 
    bake(new_data = train) %>% 
    mutate(age_source = "imputed") %>% 
    bind_rows(
        mutate(train, age_source = "original")
    ) %>% 
    filter(!is.na(age)) %>% # doesn't affect the imputed ages
    ggplot(aes(x = age, fill = age_source)) +
    geom_density(alpha = 0.25) + 
    scale_y_continuous(labels = scales::percent) +
    scale_fill_manual(values = c("original" = "#FFC42F", "imputed" = "#C77CFF")) +
    ggtitle("Age density plot")
```

The slight difference in appearance between the density plots here is reasonable, given that ages are more likely to be missing for passengers in their twenties.

## Presence of a `cabin`

We might be tempted to ignore the `cabin` value entirely, since `r scales::percent(sum(is.na(train$cabin)) / nrow(train))` of its values are missing. However, it's worth testing to see if the *presence* or *absence* of a cabin is in and of itself relevant.

```{r cabin_present_summary}
train %>% 
    mutate(cabin_present = !is.na(cabin)) %>% 
    group_by(cabin_present) %>% 
    summarise(
        passengers = n(),
        survived = scales::percent(sum(survived == 1) / passengers)
    ) %>% 
    output_table
```

The presence of cabin information is related to survival, so we'll include this binary variable as a predictor (for now). Let's set up a recipe step to do this, using the `step_mutate` function:

```{r recipe_cabin}
recipe <- recipe %>% 
    step_mutate(cabin_present = !is.na(cabin))
```

We could go further by considering the *number* of cabins associated with the passenger:

```{r number_of_cabins_summary}
train %>% 
    mutate(
        number_of_cabins = stringr::str_count(cabin, stringr::boundary("word")),
        number_of_cabins = replace_na(number_of_cabins, 0)
    ) %>% 
    group_by(number_of_cabins) %>% 
    summarise(
        passengers = n(),
        survived = scales::percent(sum(survived == 1) / passengers)
    ) %>% 
    arrange(number_of_cabins) %>% 
    output_table
```

We see here, however, that there are very few instances of passengers with more than 1 cabin (according to the available information). We'll focus our attention to the binary variable already defined for the presence of cabin information.

# Modelling efforts

Before we move onto our modelling efforts, let's prepare our recipe to apply all of the missing value imputation and feature engineering that we've done. First, let's remind ourselves what our recipe is going to do:

```{r recipe_final_summary}
recipe
```

Now let's use it! Note that we `prep` our recipe on the training data but we can `bake` it on the test set. This separates the training and test data, so that we don't have data leakage from one to the other. Our data manipulation is based entirely on the training data but can be applied to any data with the same variables.

```{r recipe_prepare_and_bake}
prepared_reciped <- recipe %>% prep(training = train, 
                                             strings_as_factors = FALSE)
train_baked <- prepared_reciped %>% bake(new_data = train)
test_baked <- prepared_reciped %>% bake(new_data = test)
```

```{r visdat_train_test_baked}
visdat::vis_dat(train_baked)
visdat::vis_dat(test_baked)
```

## Cross-validation

Let's set up some cross-validation so we can put these recipes to use. As the Kaggle-provided test set doesn't contain the target `survived` variable, we'll split the training data set into two. To avoid confusion, we'll split the training data set into `fold_train` and `fold_test` --- one of each for every fold --- to distinguih from our original train and test data sets.

We need to prepare a new recipe for every `fold_train`. Again, we'll call this `fold_recipe` to indicate that its dependent on the fold. Finally, we'll add a column for `actuals`, which is the `survived` column from `fold_test`. This will be useful when we're evaluating model performance.

```{r setting_up_folds}
folds <- train %>% vfold_cv(v = 5) %>% mutate(
    fold_recipe = map(splits, prepper, recipe = recipe, retain = TRUE),
    fold_train = map(fold_recipe, juice),
    fold_test = pmap(
        list(fold_recipe = fold_recipe, split = splits), 
        function(fold_recipe, split) bake(fold_recipe,
                                          new_data = assessment(split))
    ),
    actuals = map(fold_test, function(x) select(x, survived)) %>% list
)
folds
```

Here we see the five folds, with one row per fold. We have a column for the split used to generate the `fold_train` and `fold_test`, and a `fold_recipe`.

Now let's fit a model. With absolutely no consideration towards model selection, we'll arbitrarily choose to fit some gradient boosted decision trees. We define a `fit_xgb` function which uses the `parsnip` package as an interface to the `xgboost` model *engine*. We can then use this to fit a model for every fold, and generate predictions for every fold.

```{r xgboost}
fit_xgb <- function(data) {
    boost_tree(mode = "classification") %>%
        set_engine("xgboost") %>%
        fit(survived ~ age, data = data)
}

folds_with_xgb <- folds %>% mutate(
    xgb = map(fold_train, fit_xgb),
    preds = pmap(list(model = xgb, validate = fold_test), 
                 function(model, validate) predict(model, validate)) %>% list
) 

folds_with_xgb
```

Cross-validation without evaluation is pointless. We need to see how our five models are performing. Since we're dealing with a binary classification, we can look at three simple metrics: absolute error, false positives, and false negatives.